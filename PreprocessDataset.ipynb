{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed7bd185",
   "metadata": {},
   "source": [
    "## Preparing Loinc data for listwise search approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "37209b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndownload('en_core_web_lg')\\n\\nnlp = spacy.load('en_core_web_lg')\\n\""
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Imports\n",
    "try: \n",
    "    import pandas as pd \n",
    "    import nltk, os, re, math, spacy\n",
    "    import numpy as np\n",
    "    from spacy.cli import download\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except: \n",
    "    print(\"Install the requirements.txt\")\n",
    "\n",
    "# Requirements: Active and running version of spacy on the computer\n",
    "# !Uncomment the following part only if model isn't loaded. The download can take several minutes!\n",
    "\"\"\"\n",
    "download('en_core_web_lg')\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd68cf4",
   "metadata": {},
   "source": [
    "### Read Data from .xlsx (excel) format\n",
    "\n",
    "Since the loinc datasets are stored in excel format it is necessary to load them into dataframes for further processing tasks. We have decided to keep the given layout in order to provide future possible compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1087f008",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This functions enables the automated reading of xlsx files and the corrosponding data sheets\n",
    "#Output is a merged dataframe aswell as the queries in each datasheets and the relevancelevels of the data. \n",
    "def read_data_sheets(dataset, sheet_names):\n",
    "    queries = []\n",
    "    cols = ['loinc_num', 'long_common_name', 'component', 'system', 'property','qid', 'label']\n",
    "    df = pd.DataFrame(columns = cols)\n",
    "    for name in sheet_names:\n",
    "        query = pd.read_excel(f\"./{dataset}\",nrows = 1, sheet_name=name)\n",
    "        query = query.columns[0].split(\" \")\n",
    "        current_df = pd.read_excel(f\"./{dataset}\",skiprows=2, sheet_name=name)\n",
    "        current_df[\"query\"] = pd.Series([query[1:len(query)] for x in range(len(current_df.index))])\n",
    "        queries.append(query)\n",
    "        df = pd.concat([df, current_df],ignore_index=True, axis=0)\n",
    "    return df, queries\n",
    "\n",
    "#Function to transform Array of Format Q1 [\"This\", \"is\",\"an\", \"example\"] into a str\n",
    "def query_to_str(query) -> str: \n",
    "    q  = str()\n",
    "    for i,word in enumerate(query[1:len(query)]):\n",
    "        if(i < len(query)):\n",
    "            q = q + word+\" \"\n",
    "        else:\n",
    "            q = q + word\n",
    "    query = q\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6ffc368d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 15)\n"
     ]
    }
   ],
   "source": [
    "#Specify the datasheets that are relevant \n",
    "#Possible:  [\"glucose in blood\",\"bilirubin in plasma\",\"white blood cells count\"]\n",
    "ds_names = [\"white blood cells count\"]\n",
    "\n",
    "df, queries = read_data_sheets('loinc_dataset_extended.xlsx',ds_names)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a69a7ed",
   "metadata": {},
   "source": [
    "### Building Parameters for Listwise Approach\n",
    "\n",
    "In order to build parameters we took a close look at an already well established ranking benchmark set.\n",
    "<br/>\n",
    "Url: https://arxiv.org/ftp/arxiv/papers/1306/1306.2597.pdf (also included in the our resources)\n",
    "\n",
    "<br/>\n",
    "While the Letor 4 Ranking dataset has more than 40 parameters we reduced the amounts of parameters significantly. Main goal of this code is to show a proof of concept of the listwise approach.\n",
    "\n",
    "#### Calculating term frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8c82bf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,) (200,)\n"
     ]
    }
   ],
   "source": [
    "#Constructing Parameter => TF Score\n",
    "#The TF Score counts the overall occourcences of a word in the query in regard to the occurences per row\n",
    "\n",
    "# https://t4tutorials.com/cosine-similarity-in-data-mining/\n",
    "def termFrequency(term, document) -> float:\n",
    "  return document.count(term.lower()) / float(len(document))\n",
    "    \n",
    "\n",
    "#Idea: We compute the tf for each Loinc Entry in regard to the query. \n",
    "#Since we use an addition, there is the possibility of value exceeding 1. Therefore the data will be scaled afterwards.\n",
    "# This might be not the mathematiclly cleaneast approach but we assume that an overall high overlap in words in the query with\n",
    "# words in the row description of an loinc entry is an indicator for significance. \n",
    "def add_termFrequencies(col,name) -> pd.DataFrame:\n",
    "    tf_arr = []\n",
    "    for idx,txt in enumerate(df[col]): \n",
    "        qd_tf = 0\n",
    "        for idx, trm in enumerate(df[\"query\"][idx]): \n",
    "            #print(txt)\n",
    "            qd_tf = qd_tf + termFrequency(trm.lower(),txt.lower())\n",
    "        tf_arr.append(qd_tf)\n",
    "    return pd.DataFrame(tf_arr, columns=[name])\n",
    "\n",
    "#After the calculation of the tf scores we append them to the df as new column. \n",
    "df[\"lcmon_tf\"] =  add_termFrequencies(\"long_common_name\",\"lcmon_tf\")\n",
    "df[\"cmp_tf\"] = add_termFrequencies(\"component\",\"cmp_tf\")\n",
    "\n",
    "print(df[\"lcmon_tf\"].shape, df[\"cmp_tf\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9141e9ae",
   "metadata": {},
   "source": [
    "#### Calculating Similarity Scores\n",
    "\n",
    "The package spacy allows an already builded model to download in order to build similarity scores between text corpuses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3ed4db20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-142-1287bc9757c0>:6: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  similarity_score.append(nlp(query_to_str(dataframe[\"query\"][idx])).similarity(nlp(article)))\n"
     ]
    }
   ],
   "source": [
    "#With the use of NLP Package of spacy and the downloaded model we calculate values in the range of [0,1]. \n",
    "#As Higher the score as more similar the query and the compared text is. \n",
    "def compute_similarity_score(dataframe,column,name) -> pd.DataFrame:\n",
    "    similarity_score=[]\n",
    "    for idx,article in enumerate(dataframe[column]):\n",
    "        similarity_score.append(nlp(query_to_str(dataframe[\"query\"][idx])).similarity(nlp(article)))\n",
    "    return pd.DataFrame(similarity_score, columns=[name])\n",
    "\n",
    "df[\"lcmon_ss\"] = compute_similarity_score(df, \"long_common_name\", \"lcmon_ss\")\n",
    "df[\"cmp_ss\"] = compute_similarity_score(df, \"component\", \"cmp_ss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d0abbc",
   "metadata": {},
   "source": [
    "#### Description for identifying ranking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "eedd988f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "#In order to identify the results of our predictions we need an identifier. Since Loinc already uses its own identifiers,\n",
    "#a special loinc number, we use the same number for identification. \n",
    "description = []\n",
    "for idx in range(len(df)):\n",
    "    description.append(f' #docid = {df.loinc_num[idx]} {df.long_common_name[idx]}')\n",
    "print(len(description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5f5b2c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    label qid  lcmon_ss  lcmon_tf  cmp_tf    cmp_ss\n",
      "0       0   3  0.488962       0.0     0.0  0.368049\n",
      "1       0   3  0.687383       0.0     0.0  0.523740\n",
      "2       0   3  0.555352       0.0     0.0  0.368049\n",
      "3       0   3  0.478199       0.0     0.0  0.362320\n",
      "4       0   3  0.335934       0.0     0.0  0.160475\n",
      "..    ...  ..       ...       ...     ...       ...\n",
      "195     0   3  0.547386       0.0     0.0  0.472740\n",
      "196     0   3  0.405370       0.0     0.0  0.472740\n",
      "197     0   3  0.512250       0.0     0.0  0.472740\n",
      "198     0   3  0.439458       0.0     0.0  0.472740\n",
      "199     0   3  0.547004       0.0     0.0  0.472740\n",
      "\n",
      "[200 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#All columns that we use for the listwise ranking algorithm adaRank. \n",
    "prep_df = df[[\"label\",\"qid\",\"lcmon_ss\",\"lcmon_tf\",\"cmp_tf\",\"cmp_ss\"]] \n",
    "print(prep_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04283164",
   "metadata": {},
   "source": [
    "### Scaling Data\n",
    "\n",
    "Since we used an unorthodoxed approach by just adding the tf scores together, we need Scale the values. Otherwise the difference between the similartiy scores and the aggregated tf scores could lead to a biased result. Furthermore we scale all numerical values with MinMaxScaler. MinMaxScaler dosent treat outliers very well. But our assesment is, that outliers represent a very high similarity and therefore dont need to be avoided because they likely indicate a very good match between query and searched Loinc entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9fb87f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['label', 'qid', 'lcmon_ss', 'lcmon_tf', 'cmp_tf', 'cmp_ss'], dtype='object')\n",
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200, 6)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "cols = list(prep_df[[\"lcmon_ss\",\"lcmon_tf\",\"cmp_tf\",\"cmp_ss\"]])\n",
    "prep_df.values[:] = MinMaxScaler().fit_transform(prep_df)\n",
    "prep_df = pd.DataFrame(prep_df)\n",
    "print(prep_df.columns)\n",
    "print(len(prep_df))\n",
    "\n",
    "prep_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7514f9c6",
   "metadata": {},
   "source": [
    "### Train / Test / Validation Split and converting data into sparse matrix\n",
    "\n",
    "Since we are using the implementation by the github user: https://github.com/rueycheng/AdaRank the format of an sparse matrix is required.\n",
    "\n",
    "#### Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6fefb002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test Split. After taking the train test split we split the test set another time in order to get a test and \n",
    "#a validation set\n",
    "X = prep_df[[\"qid\",\"lcmon_ss\",\"lcmon_tf\",\"cmp_tf\",\"cmp_ss\"]]\n",
    "y = prep_df[[\"label\"]].values.ravel()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a71b06",
   "metadata": {},
   "source": [
    "#### Dump to svmlight file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2f338068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import dump_svmlight_file\n",
    "\n",
    "#This function adds the identifier als already described to the svmlight outputed textfile in order to identify the ranked\n",
    "#documents\n",
    "def add_comments(description, indexes, file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        file_lines = [''.join([x.strip(), description[indexes[idx]], '\\n']) for idx,x in enumerate(f.readlines())]\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.writelines(file_lines) \n",
    "\n",
    "#Function to save the preprocessed dataframe into svmlight format\n",
    "def dump_to_svmlf(df,y,name,descriptio): \n",
    "    X = df[[\"lcmon_ss\",\"lcmon_tf\",\"cmp_tf\",\"cmp_ss\"]]\n",
    "    qid = df[[\"qid\"]].values.ravel()\n",
    "    y = y.values.ravel()\n",
    "    f = f\"./{name}.txt\"\n",
    "    #print(y.shape)\n",
    "    #print(len(X),len(y),len(qid))\n",
    "    dump_svmlight_file(X, y,f, zero_based=False, comment=None, query_id=qid, multilabel=False)\n",
    "    add_comments(description, X.index.values,f)\n",
    "\n",
    "    \n",
    "def save_to_folder(folder_path):\n",
    "    dump_to_svmlf(X_train,pd.DataFrame(y_train),f\"data/{folder_path}/train\",description)\n",
    "    dump_to_svmlf(X_test,pd.DataFrame(y_test),f\"data/{folder_path}/test\",description)\n",
    "    dump_to_svmlf(X_val,pd.DataFrame(y_val),f\"data/{folder_path}/vali\",description)\n",
    "\n",
    "\n",
    "#Change regarding the  used query and used dataset\n",
    "save_to_folder(\"extend_ds/q3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5c89ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
